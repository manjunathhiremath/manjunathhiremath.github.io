---
layout: default
title: LLM Settings
parent: Basics of Prompt Engineering
nav_order: 3
---

# LLM Settings

When designing and testing prompts, you typically interact with the LLM via an API. You can configure a few parameters to get different results for your prompts. Tweaking these settings are important to improve reliability and desirability of responses and it takes a bit of experimentation to figure out the proper settings for your use cases. Below are the common settings you will come across when using different LLM providers:

## 1.Temperature
In short, the lower the temperature, the more deterministic the results in the sense that the highest probable next token is always picked. Increasing temperature could lead to more randomness, which encourages more diverse or creative outputs. You are essentially increasing the weights of the other possible tokens. In terms of application, you might want to use a lower temperature value for tasks like fact-based QA to encourage more factual and concise responses. For poem generation or other creative tasks, it might be beneficial to increase the temperature value.

### Low Temperature (e.g., 0.1 - 0.4)
- **Deterministic and Fact-Based Tasks**: When you want the model to provide consistent, precise answers, such as for:
  - Code generation or debugging
  - Answering factual questions (e.g., scientific information, mathematical calculations)
  - Summarizing text with minimal variability
- **Formal and Structured Outputs**: For tasks requiring structure, like generating legal or technical documents where precision and consistency are crucial.

### High Temperature (e.g., 0.7 - 1.2)
- **Creative Writing or Brainstorming**: When you want diverse or imaginative output, such as:
  - Writing stories, poems, or creative pieces
  - Generating multiple ideas or variations for brainstorming
- **Conversational Chatbots**: For chatbots that need to sound natural, engaging, and varied in conversation.
- **Exploratory Outputs**: When experimenting with different responses, such as generating variations in marketing slogans or ad copies.

### Medium Temperature (e.g., 0.5 - 0.7)
- This range strikes a balance, making it suitable for cases where you want some creativity but still need reasonable consistency, like casual conversation or generating informative but engaging text.

## 2.Top P
A sampling technique with temperature, called nucleus sampling, where you can control how deterministic the model is. If you are looking for exact and factual answers keep this low. If you are looking for more diverse responses, increase to a higher value. If you use Top P it means that only the tokens comprising the top_p probability mass are considered for responses, so a low top_p value selects the most confident responses. This means that a high top_p value will enable the model to look at more possible words, including less likely ones, leading to more diverse outputs.

**The general recommendation is to alter temperature or Top P but not both.**

## 3.Max Length
You can manage the number of tokens the model generates by adjusting the max length. Specifying a max length helps you prevent long or irrelevant responses and control costs.
Yes, the token limit **does** include the system prompt. In LLMs, all components of the input—such as the system prompt, user input, and any other context—count towards the total token limit. Here's how it breaks down:

### Components Included in the Token Limit
1. **System Prompt**: This is the initial instruction or configuration set by the system to guide the behavior of the model. It defines how the model should respond (e.g., "You are a helpful assistant").
   
2. **User Input**: The actual prompt or question provided by the user. This is the main content that the model processes in conjunction with the system prompt.

3. **Model's Response**: The output generated by the model also contributes to the token count.

4. **Special Tokens**: Any start, end, or separator tokens that the model might use to structure the input and output.

### Example
If the system prompt uses 50 tokens, and the user input takes 100 tokens, that adds up to 150 tokens. If the model's max length is 4,096 tokens, this leaves 3,946 tokens for the model’s response.

### Summary
The entire sequence—**system prompt, user prompt, and generated response**—must stay within the model's token limit.

## 4.Stop Sequences 
A stop sequence is a string that stops the model from generating tokens. Specifying stop sequences is another way to control the length and structure of the model's response. For example, you can tell the model to generate lists that have no more than 10 items by adding "11" as a stop sequence.

## 5.Frequency Penalty
The frequency penalty applies a penalty on the next token proportional to how many times that token already appeared in the response and prompt. The higher the frequency penalty, the less likely a word will appear again. This setting reduces the repetition of words in the model's response by giving tokens that appear more a higher penalty.

## 6.Presence Penalty
The presence penalty also applies a penalty on repeated tokens but, unlike the frequency penalty, the penalty is the same for all repeated tokens. A token that appears twice and a token that appears 10 times are penalized the same. This setting prevents the model from repeating phrases too often in its response. If you want the model to generate diverse or creative text, you might want to use a higher presence penalty. Or, if you need the model to stay focused, try using a lower presence penalty.

**Similar to temperature and top_p, the general recommendation is to alter the frequency or presence penalty but not both.**

**Before starting with some basic examples, keep in mind that your results may vary depending on the version of LLM you use.**
